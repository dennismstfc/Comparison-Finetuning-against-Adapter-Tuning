# Experimental setup to examine the impact of adapter tuning of the BERT architecture with LexGLUE
In my bachelor thesis I dealt with the determination of the impact of adding adapter modules to the BERT architecture. As part of this work, I implemented an empirical study that compares training methods and determines the resulting differences. This repository contains the source code that performs the training of the BERT architecture using finetuning on the one hand and adapter tuning on the other. Adapter modules with the bottleneck architecture were used for adapter tuning. 
